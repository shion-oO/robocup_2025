<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>障害物</title>
  <style>
    body {
      font-family: 'Helvetica Neue', sans-serif;
      background-color: #f0f4f8;
      margin: 0;
      padding: 0;
      color: #2c3e50;
    }

    header {
      background-color: #3498db;
      color: white;
      padding: 20px;
      text-align: center;
    }

    h1 {
      margin: 0;
      font-size: 2em;
    }

    nav {
      background-color: #ecf0f1;
      padding: 10px 20px;
      text-align: center;
    }

    nav a {
      color: #3498db;
      text-decoration: none;
      margin: 0 15px;
      font-weight: bold;
    }

    nav a:hover {
      text-decoration: underline;
    }

    main {
      padding: 40px 20px;
      max-width: 1200px;
      margin: 0 auto;
      background-color: #ffffff;
      border-radius: 10px;
      box-shadow: 0 4px 10px rgba(0,0,0,0.1);
    }

    video {
      width: 100%;
      max-width: 640px;
      border-radius: 10px;
      display: block;
      margin: 0 auto;
    }

    .caption {
      margin-top: 12px;
      font-size: 16px;
      color: #444;
      text-align: center;
    }

    h2 {
      color: #3498db;
      border-bottom: 2px solid #3498db;
      padding-bottom: 5px;
      margin-top: 40px;
    }

    .code-section {
      display: flex;
      gap: 20px;
      margin-top: 20px;
      flex-wrap: wrap;
    }

    .code-box {
      flex: 1;
      min-width: 400px;
      background: #f7f9fb;
      border-radius: 10px;
      padding: 20px;
      overflow-x: auto;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }

    pre {
      margin: 0;
      font-family: 'Courier New', monospace;
      font-size: 14px;
      line-height: 1.5;
      color: #333;
    }

    .desc-box {
      flex: 1;
      min-width: 400px;
      background: #eaf4fc;
      border-radius: 10px;
      padding: 20px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }

    .desc-box p {
      margin-bottom: 10px;
      line-height: 1.6;
    }

    /* ▼ 学習セクション ▼ */
    .learn-section {
      margin-top: 60px;
      background-color: #f7f9fb;
      padding: 40px 30px;
      border-radius: 10px;
      box-shadow: 0 3px 8px rgba(0,0,0,0.1);
    }

    .learn-section h2 {
      text-align: center;
      font-size: 1.8em;
      color: #3498db;
      border-bottom: none;
      margin-bottom: 20px;
    }

    .learn-section p {
      font-size: 1.1em;
      line-height: 1.8;
      color: #2c3e50;
      margin-bottom: 15px;
    }

    .learn-list {
      list-style: decimal;
      padding-left: 25px;
      font-size: 1.05em;
      line-height: 1.8;
    }

  </style>
</head>
<body>

  <header>
    <h1>障害物</h1>
  </header>

  <nav>
    <a href="robocup_home.html">ホーム</a>
    <a href="speedbanmp.html">スピードバンプ</a>
    <a href="gareki.html">瓦礫</a>
    <a href="seesaw.html">シーソー</a>
    <a href="keisyaro.html">傾斜路</a>
  </nav>

  <main>
    <video controls>
      <source src="syougaibutu.mp4" type="video/mp4">
      お使いのブラウザは動画再生に対応していません。
    </video>
    <div class="caption">
    ロボットが障害物を避けて走行する様子を撮影した動画です。
    </div>

    <h2>choonpa_only_shougaibutu.py</h2>
    <div class="code-section">
      <div class="code-box">
        <pre><code>import cv2
import numpy as np
from picamera2 import Picamera2
from etrobo_python import ETRobo, Motor, SonarSensor
import time

# --- カメラの初期設定 ---
cam0 = Picamera2(0)
cam0.configure(cam0.create_preview_configuration(main={"format": 'XRGB8888', "size": (320, 240)}))
cam0.start()
time.sleep(1)

cam1 = Picamera2(1)
cam1.configure(cam1.create_preview_configuration(main={"format": 'XRGB8888', "size": (320, 240)}))
cam1.start()
time.sleep(1)

last_direction = "CENTER"
avoid_mode = False  # 障害物回避モードフラグ

# --- ラインの位置から進行方向を判定する関数 ---
def get_direction(frame, last_direction):
    hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)
    lower_black = (0, 0, 0)
    upper_black = (180, 255, 80)
    mask = cv2.inRange(hsv, lower_black, upper_black)
    mask = cv2.GaussianBlur(mask, (5, 5), 0)

    height, width = mask.shape
    roi_height = 40
    roi_bottom = mask[height - roi_height:height, :]  # 下部のROI（ライン検出用領域）
    roi_debug = cv2.cvtColor(mask.copy(), cv2.COLOR_GRAY2BGR)

    contours, _ = cv2.findContours(roi_bottom, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cx_list = []
    for c in contours:
        if cv2.contourArea(c) > 400:  # 小さいノイズを除去
            M = cv2.moments(c)
            if M["m00"] != 0:
                cx = int(M["m10"] / M["m00"])
                cy = int(M["m01"] / M["m00"])
                cx_list.append(cx)
                cv2.circle(roi_debug, (cx, cy + height - roi_height), 5, (0, 255, 0), -1)

    direction = last_direction
    tolerance = 20  # 許容範囲
    if cx_list:
        avg_cx = sum(cx_list) // len(cx_list)
        if avg_cx < width // 2 - tolerance:
            direction = "LEFT"   # 左へ
        elif avg_cx > width // 2 + tolerance:
            direction = "RIGHT"  # 右へ
        else:
            direction = "CENTER" # まっすぐ
    else:
        direction = last_direction  # ラインが見えないときは前回の方向を維持

    return direction, roi_debug

# --- ライントレースと障害物回避を同時に行うメイン関数 ---
def line_trace_and_avoid(right_motor: Motor, left_motor: Motor, sonar: SonarSensor):
    global last_direction, avoid_mode

    try:
        distance = sonar.get_distance()
        print(f"Sonar distance: {distance} mm")

        if avoid_mode:
            # 障害物を避けたあとの走行経路（避け終わり動作）
            left_motor.set_power(60)
            right_motor.set_power(30)
            time.sleep(1.3)

            left_motor.set_power(30)
            right_motor.set_power(60)
            time.sleep(1.1)

            avoid_mode = False
            return

        if distance < 270:
            print("[AVOID] 障害物を検出！ 回避動作を開始します...")
            avoid_mode = True
            return

        # カメラから映像を取得
        frame0 = cam0.capture_array()
        frame1 = cam1.capture_array()

        direction, debug_mask0 = get_direction(frame0, last_direction)
        _, debug_mask1 = get_direction(frame1, last_direction)
        last_direction = direction

        print("Direction:", direction)

        # 基本速度を設定（方向により調整）
        base = 35 if direction == "CENTER" else 30
        diff = 25

        if direction == "LEFT":
            right_motor.set_power(base + diff)
            left_motor.set_power(base - diff)
        elif direction == "RIGHT":
            right_motor.set_power(base - diff)
            left_motor.set_power(base + diff)
        else:
            right_motor.set_power(base)
            left_motor.set_power(base)

        # カメラ映像を表示（デバッグ用）
        cv2.imshow("Cam0 Mask", debug_mask0)
        cv2.imshow("Cam1 Mask", debug_mask1)
        cv2.waitKey(1)

    except Exception as e:
        print("[ERROR]:", e)

# --- 実行部分 ---
(ETRobo(backend='raspyke')
 .add_device('right_motor', device_type=Motor, port='B')
 .add_device('left_motor', device_type=Motor, port='C')
 .add_device('sonar', device_type=SonarSensor, port='S1')
 .add_handler(line_trace_and_avoid)
 .dispatch(interval=0.05))

</code></pre>
      </div>

<div class="desc-box">
  <h3>(プログラムの説明)</h3>

  <p>
1. ライブラリの読み込み（道具箱を用意する部分）<br><br>

cv2（OpenCV）：画像処理のため。色変換、マスク作成、輪郭検出などをします。<br>
numpy（np）：配列計算を便利にする。画像は配列として扱います。<br>
Picamera2：Raspberry Pi のカメラを扱うためのライブラリ。<br>
ETRobo / Motor / SonarSensor：ロボット本体・モーター・超音波センサーを動かすためのライブラリ。<br>
time：待ち時間（sleep）に使います。<br><br>


2. カメラの初期設定（cam0, cam1）<br><br>

Picamera2(0) と Picamera2(1)：2台のカメラを想定（0 が1台目、1 が2台目）。<br>
size=(320,240)：画像サイズを小さくして処理を速くしています（小さいと軽いが、細かい線は見えにくい）。<br>
time.sleep(1)：カメラの起動待ち。カメラは起動直後に明るさや色が安定しないことがあるので待ちます。<br>
注意点：実機で1台しかない場合は cam1 の設定を消す（もしくはエラーになるので適切に対応）必要があります。<br>
last_direction：前回の進行方向（"LEFT" / "RIGHT" / "CENTER"）を覚えておきます。線が見えない瞬間の挙動安定化に使う。<br>
avoid_mode：障害物回避中かどうかを示すフラグ（True/False）。<br><br>


3. get_direction 関数 — 画像からラインの方向を推定する（超重要）<br><br>

関数の全体像：カメラ画像を受け取って「左／中央／右」のどれかを返します。ついでにデバッグ用マスク画像も返す。<br><br>

詳しい解説（ステップごと）<br><br>

色空間の変換（RGB → HSV）<br>
hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)<br><br>

HSVは色（H）・彩度（S）・明るさ（V）で表現され、色や明るさの条件でフィルタするのに扱いやすいです。<br><br>

黒色の領域だけ取り出す（マスク作成）<br>
lower_black = (0, 0, 0)<br>
upper_black = (180, 255, 80)<br>
mask = cv2.inRange(hsv, lower_black, upper_black)<br><br>

明るさ（V）の上限を80にして暗い（＝黒っぽい）部分を抽出しています。<br>
inRange の結果は白（線）と黒（背景）の二値画像になります。<br><br>

ぼかし（ノイズ除去）<br>
mask = cv2.GaussianBlur(mask, (5, 5), 0)<br><br>

小さな点ノイズを減らして輪郭検出を安定させます。<br><br>

ROI（下の40ピクセルだけ見る）<br>
height, width = mask.shape<br>
roi_height = 40<br>
roi_bottom = mask[height - roi_height:height, :]<br><br>

ロボットの視点だとラインは画面の下の方に映るので、下だけ見れば十分で処理が速くなります。<br><br>

輪郭検出<br>
contours, _ = cv2.findContours(roi_bottom, ...)<br><br>

findContours で白い部分（＝黒線がある箇所）の形を見つけます。<br><br>

重心（cx）を計算して位置を得る<br>
if cv2.contourArea(c) > 400:<br>
&nbsp;&nbsp;&nbsp;&nbsp;M = cv2.moments(c)<br>
&nbsp;&nbsp;&nbsp;&nbsp;cx = int(M["m10"] / M["m00"])<br><br>

小さい面積（例：ゴミ）を無視するために area > 400 のフィルタ。<br>
moments から重心（cx）を得て、どの横位置にあるかをリストに追加します。<br><br>

平均cxで左右判定<br>
avg_cx = sum(cx_list) // len(cx_list)<br>
tolerance = 20<br>
if avg_cx < width // 2 - tolerance: direction = "LEFT"<br>
elif avg_cx > width // 2 + tolerance: direction = "RIGHT"<br>
else: direction = "CENTER"<br><br>

画面中央（width//2）からのズレを見て左/右/中央を判定。toleranceで中央判定の余裕を作っています。<br><br>

ラインが見えない場合<br>
else:<br>
&nbsp;&nbsp;&nbsp;&nbsp;direction = last_direction<br><br>

見えないときは直前の方向を使って動作を続け、ふらつきを減らします。<br><br>


4. line_trace_and_avoid 関数 — ライントレースと障害物回避を組み合わせたメイン処理<br><br>

この関数は ETRobo のハンドラとして定期的に呼ばれ、モーターとセンサーを操作します。<br><br>

処理の流れ（順を追って）<br><br>

超音波センサーで距離を測る<br>
distance = sonar.get_distance()<br><br>

前方の障害物（人や箱など）までの距離（mm）を取得します。<br><br>

回避中なら「回避後の復帰動作」を行う<br>
if avoid_mode:<br>
&nbsp;&nbsp;&nbsp;&nbsp;left_motor.set_power(60)<br>
&nbsp;&nbsp;&nbsp;&nbsp;right_motor.set_power(30)<br>
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1.3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;left_motor.set_power(30)<br>
&nbsp;&nbsp;&nbsp;&nbsp;right_motor.set_power(60)<br>
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1.1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;avoid_mode = False<br>
&nbsp;&nbsp;&nbsp;&nbsp;return<br><br>

avoid_mode が True のときは、事前に決めた一連のモーター操作（左右の速度差をつける）で復帰動作を行います。<br>
このブロックは「回避完了後に元のラインに戻るための動き」と考えるとわかりやすいです。<br><br>

もし距離が閾値（ここでは270mm）より近ければ回避開始<br>
if distance < 270:<br>
&nbsp;&nbsp;&nbsp;&nbsp;avoid_mode = True<br>
&nbsp;&nbsp;&nbsp;&nbsp;return<br><br>

障害物が近い → avoid_mode = True にして次ループで回避動作へ移ります。<br><br>

カメラ映像を取得して方向判定<br>
frame0 = cam0.capture_array()<br>
frame1 = cam1.capture_array()<br>
direction, debug_mask0 = get_direction(frame0, last_direction)<br>
_, debug_mask1 = get_direction(frame1, last_direction)<br>
last_direction = direction<br><br>

2台のカメラから画像を取得し、get_direction により方向を得ます。last_direction を更新。<br>
なぜ2台？：片方が見えにくい時の冗長性や、両方から判定してより安定させる目的。ただし現在のコードはcam0の結果しか方向に使っていない点に注意（改良余地あり）。<br><br>

モーターへ出力（走行制御）<br>
base = 35 if direction == "CENTER" else 30<br>
diff = 25<br>
if direction == "LEFT":<br>
&nbsp;&nbsp;&nbsp;&nbsp;right_motor.set_power(base + diff)<br>
&nbsp;&nbsp;&nbsp;&nbsp;left_motor.set_power(base - diff)<br>
...<br><br>

base と diff を使って左右のパワー差をつけ、曲がりやすさを調整します。<br>
例：direction == "LEFT" のとき、右側のモーターを速くして左へ曲がる。<br><br>

デバッグ表示<br>
cv2.imshow("Cam0 Mask", debug_mask0)<br>
cv2.imshow("Cam1 Mask", debug_mask1)<br>
cv2.waitKey(1)<br><br>

マスク画像を画面に表示して、どこが黒線として検出されているかを確認できます。実機デバッグで非常に便利。<br><br>

例外処理<br>
except Exception as e:<br>
&nbsp;&nbsp;&nbsp;&nbsp;print("[ERROR]:", e)<br><br>

何かエラーが発生したときにメッセージを出して止まらないようにします。<br><br>


5. 実行部分（ETRoboの設定）<br><br>

add_device：モーターやセンサーの登録（右モーターをBポート、左をCポート、ソナーをS1ポートに接続）。<br>
add_handler(line_trace_and_avoid)：先ほどの関数を定期的に実行するよう登録。<br>
dispatch(interval=0.05)：0.05秒（50ミリ秒）ごとに line_trace_and_avoid を呼ぶ。これがロボットの制御ループの周期です（20Hz）。<br>
  </p>
</div>


<div class="learn-section">
  <h2>📘 学習のやり方</h2>
  <p>障害物回避とライントレースを理解するための流れ：</p>
  <ol class="learn-list">
    <li>カメラで床の黒線を見つける</li>
    <li>線の位置からロボットの進む方向を判断する</li>
    <li>左右のモーター速度を変えて方向転換する</li>
    <li>超音波センサーで障害物を検知 → 回避動作</li>
    <li>これを繰り返し → ロボットが自律的に走行</li>
  </ol>
  <p>実際にロボットを動かすと、<strong>「センサー」と「画像処理」のつながり</strong>が自然と理解できるようになります。</p>
</div>


  </main>

</body>
</html>
